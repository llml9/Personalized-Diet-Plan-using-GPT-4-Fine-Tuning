# -*- coding: utf-8 -*-
"""gpt4_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NmZbdS3XIg_eflMITn7sH8kDv0gKORgT
"""

!pip install openai -q

!pip install pycocoevalcap
!pip install bert_score

!pip install streamlit
!pip install evaluate
!pip install rouge_score

from sklearn.model_selection import train_test_split
import pandas as pd

file_path = "/content/cleaned_diet_rephrased_only (1).csv"
df = pd.read_csv(file_path)

train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)

train_df.to_csv("diet_train.csv", index=False)
val_df.to_csv("diet_validation.csv", index=False)

import json
import pandas as pd
from sklearn.model_selection import train_test_split
import random

# Paths
train_jsonl_path = "/content/diet_train.jsonl"
val_jsonl_path = "/content/diet_validation.jsonl"

# Function to randomly rephrase the answer
def vary_completion_style(text):
    styles = [
        lambda x: x,
        lambda x: x.replace("‚Ä¢", "-").replace("*", "-"),
        lambda x: x.replace("Diet:", "Nutrition Plan:").replace("Calories:", "Energy Intake:"),
        lambda x: "Here is your tailored health plan:\n" + x,
    ]
    return random.choice(styles)(text)

# Generate fewer prompts with different phrasings
def create_multiple_prompts(row):
    gender = "male" if row['Gender'] == "male" else "female"
    base_info = (
        f"A {gender}, {row['Age']} years old, {row['Height_cm']} cm, "
        f"weighs {row['Weight_kg']} kg, BMI: {row['BMI']}, "
        f"suffers from {row['Disease_Type']} (Severity: {row['Severity']}), "
        f"activity level: {row['Physical_Activity_Level']}, "
        f"dietary restrictions: {row['Dietary_Restrictions']}, "
        f"allergies: {row['Allergies']}, prefers {row['Preferred_Cuisine']} food."
    )

    prompts = [
        f"Generate a full meal and fitness strategy for this profile: {base_info}",
        f"I'm seeking a health plan based on these details: {base_info}",
        f"Design a weekly nutrition and physical activity guide for: {base_info}"
    ]

    examples = []
    for prompt in prompts:
        examples.append({
            "messages": [
                {"role": "user", "content": prompt},
                {"role": "assistant", "content": vary_completion_style(row["Personalized_Diet_Plan"])}
            ]
        })
    return examples

# Save training data
with open(train_jsonl_path, 'w', encoding='utf-8') as f:
    for _, row in train_df.iterrows():
        for example in create_multiple_prompts(row):
            json.dump(example, f, ensure_ascii=False)
            f.write('\n')

with open(val_jsonl_path, 'w', encoding='utf-8') as f:
    for _, row in val_df.iterrows():
        json.dump(create_multiple_prompts(row)[0], f, ensure_ascii=False)
        f.write('\n')

print("‚úÖ JSONL updated with 3 varied prompts and rephrased completions!")

import os
from openai import OpenAI
from time import sleep

# Initialize OpenAI client
client = OpenAI(api_key = "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")

def upload_training_file(file_path):
    """Upload training file to OpenAI"""
    with open(file_path, "rb") as file:
        response = client.files.create(
            file=file,
            purpose="fine-tune"
        )
        return response.id

training_file_id = upload_training_file("/content/diet_train (1).jsonl")
validation_file_id = upload_training_file("/content/diet_validation (3).jsonl")
training_file_id,validation_file_id

def create_fine_tuning_job(training_file_id, validation_file_id=None, model="gpt-4o-mini-2024-07-18"):
    """Create a fine-tuning job"""
    response = client.fine_tuning.jobs.create(
        training_file=training_file_id,
        validation_file=validation_file_id,
        model=model,
        hyperparameters={
            "n_epochs": 3,
            "batch_size": 2,

        }
    )
    return response.id

model="gpt-4o-mini-2024-07-18"
job_id = create_fine_tuning_job(training_file_id, validation_file_id, model)
job_id

def monitor_job(job_id):
    """Monitor fine-tuning job progress"""
    while True:
        job = client.fine_tuning.jobs.retrieve(job_id)
        print(f"Status: {job.status}")

        if job.status in ["succeeded", "failed"]:
            return job

        # List latest events
        events = client.fine_tuning.jobs.list_events(
            fine_tuning_job_id=job_id,
            limit=5
        )
        for event in events.data:
            print(f"Event: {event.message}")

        sleep(30)  # Check every 30 seconds

job =  monitor_job(job_id)
if job.status == "succeeded":
  fine_tuned_model = job.fine_tuned_model
  print(f"Fine-tuned model ID: {fine_tuned_model}")
else:
  print("Fine-tuning failed.")

import openai
def test_model(model_id, test_input):
    """Test the fine-tuned model"""
    completion = client.chat.completions.create(
        model=model_id,
        messages=[
            {
                "role": "system",
                "content": ""
            },
            {"role": "user", "content": test_input}
        ]
    )
    return completion.choices[0].message

fine_tuned_model='ft:gpt-4o-mini-2024-07-18:meead-sultan::BN4X6bXE'

custom_test_input = "Create a diet plan for a 25-year-old female with high cholesterol, preferring Mediterranean cuisine."
result = test_model(fine_tuned_model, custom_test_input)

result.content

# Commented out IPython magic to ensure Python compatibility.
# %%writefile gpt444.py
# import streamlit as st
# import openai
# import os
# 
# import openai
# 
# openai.api_key = "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"
# 
# 
# 
# # Fine-Tuned Model ID
# fine_tuned_model = "ft:gpt-4o-mini-2024-07-18:meead-sultan::BN4X6bXE"
# 
# # Initialize Streamlit UI
# st.title("üçè AI Diet Plan Generator")
# st.write("Enter patient details to generate a personalized diet plan.")
# 
# # User Input
# user_input = st.text_input("üí¨ Enter patient details (e.g., Create a diet plan for a 30-year-old male with diabetes)")
# 
# # Function to Generate Diet Plan Using Streaming
# def generate_diet_plan(user_prompt):
#     try:
#         response = openai.ChatCompletion.create(
#             model=fine_tuned_model,
#             messages=[
#                 {"role": "system", "content": "You are a professional dietitian and fitness coach."},
#                 {"role": "user", "content": user_prompt}
#             ],
#             stream=True  # Enable Streaming
#         )
# 
#         # Return streamed text output
#         for chunk in response:
#             if "choices" in chunk and chunk["choices"]:
#                 yield chunk["choices"][0]["delta"].get("content", "")
# 
#     except openai.error.AuthenticationError:
#         yield "‚ùå **Authentication Error: Invalid API Key.** Please check your API key and try again."
# 
# # Generate Button
# if st.button("üîç Generate Diet Plan"):
#     if user_input:
#         st.write("‚è≥ **Generating diet plan...**")
# 
#         # Display streamed text
#         diet_plan_text = st.empty()
#         full_text = ""
# 
#         for text_chunk in generate_diet_plan(user_input):
#             full_text += text_chunk
#             diet_plan_text.markdown(full_text)  # Update text in Streamlit
# 
#         st.success("‚úÖ Diet plan generated successfully!")
#     else:
#         st.warning("‚ö†Ô∏è Please enter patient details!")
#

import json
from openai import OpenAI
import time

validation_path = "/content/diet_validation (3).jsonl"
prompts = []
references = []

with open(validation_path, "r", encoding="utf-8") as f:
    for line in f:
        data = json.loads(line)
        user_msg = None
        assistant_msg = None
        for msg in data["messages"]:
            if msg["role"] == "user":
                user_msg = msg["content"]
            elif msg["role"] == "assistant":
                assistant_msg = msg["content"]
        if user_msg and assistant_msg:
            prompts.append(user_msg)
            references.append(assistant_msg)

client = OpenAI(api_key="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")

predictions = []

for i, prompt in enumerate(prompts):
    response = client.chat.completions.create(
        model="ft:gpt-4o-mini-2024-07-18:meead-sultan::BN4X6bXE",
        messages=[{"role": "user", "content": prompt}],
        temperature=0
    )
    answer = response.choices[0].message.content.strip()
    predictions.append(answer)
    print(f"[{i+1}/{len(prompts)}] ‚úÖ")

    time.sleep(0.5)

import evaluate

bleu = evaluate.load("bleu")
bleu_score = bleu.compute(predictions=predictions, references=[[ref] for ref in references])
print("\n BLEU Score:", bleu_score["bleu"])

rouge = evaluate.load("rouge")
rouge_score = rouge.compute(predictions=predictions, references=references)

print("\n ROUGE Scores:")
print("ROUGE-1:", rouge_score["rouge1"])
print("ROUGE-2:", rouge_score["rouge2"])
print("ROUGE-L:", rouge_score["rougeL"])
print("ROUGE-Lsum:", rouge_score["rougeLsum"])

import evaluate

bertscore = evaluate.load("bertscore")
score = bertscore.compute(predictions=predictions, references=references, lang="en")
print("BERTScore - Precision:", sum(score["precision"]) / len(score["precision"]))
print("BERTScore - Recall:", sum(score["recall"]) / len(score["recall"]))
print("BERTScore - F1:", sum(score["f1"]) / len(score["f1"]))

import json
import evaluate
from openai import OpenAI

test_path = "/content/diet_test_10_groud.jsonl"
promptsTest = []
referencesTest = []

with open(test_path, "r", encoding="utf-8") as f:
    for line in f:
        data = json.loads(line)
        messages = data["messages"]
        for msg in messages:
            if msg["role"] == "user":
                promptsTest.append(msg["content"])
            elif msg["role"] == "assistant":
                referencesTest.append(msg["content"])

client = OpenAI(api_key="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")

predictionsTest = []

for prompt in promptsTest:
    response = client.chat.completions.create(
        model="ft:gpt-4o-mini-2024-07-18:meead-sultan::BN4X6bXE",
        messages=[{"role": "user", "content": prompt}],
        temperature=0
    )
    answer = response.choices[0].message.content.strip()
    predictionsTest.append(answer)

-bertscore = evaluate.load("bertscore")
score = bertscore.compute(predictions=predictionsTest, references=referencesTest, lang="en")

print("BERTScore - Precision:", sum(score["precision"]) / len(score["precision"]))
print("BERTScore - Recall:", sum(score["recall"]) / len(score["recall"]))
print("BERTScore - F1:", sum(score["f1"]) / len(score["f1"]))

import evaluate
import matplotlib.pyplot as plt

bertscore = evaluate.load("bertscore")
val_score = bertscore.compute(predictions=predictions, references=references, lang="en")
val_precision = sum(val_score["precision"]) / len(val_score["precision"])
val_recall = sum(val_score["recall"]) / len(val_score["recall"])
val_f1 = sum(val_score["f1"]) / len(val_score["f1"])

test_score = bertscore.compute(predictions=predictionsTest, references=referencesTest, lang="en")
test_precision = sum(test_score["precision"]) / len(test_score["precision"])
test_recall = sum(test_score["recall"]) / len(test_score["recall"])
test_f1 = sum(test_score["f1"]) / len(test_score["f1"])

labels = ["Precision", "Recall", "F1"]
val_values = [val_precision, val_recall, val_f1]
test_values = [test_precision, test_recall, test_f1]

x = range(len(labels))
bar_width = 0.4

plt.figure(figsize=(8, 5))
bars1 = plt.bar([i - 0.2 for i in x], val_values, width=bar_width, label="Validation", color="#4c72b0")
bars2 = plt.bar([i + 0.2 for i in x], test_values, width=bar_width, label="Test", color="#f28e2c")

for bars in [bars1, bars2]:
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width() / 2, height + 0.0001, f"{height:.4f}", ha="center", fontsize=9)

plt.xticks(x, labels)
plt.title("BERTScore Comparison (Validation vs Test)")
plt.ylabel("Score")
plt.ylim(0.90, 1.001)
plt.legend()
plt.grid(axis="y", linestyle="--", alpha=0.4)
plt.tight_layout()
plt.show()

from pycocoevalcap.meteor.meteor import Meteor
import evaluate

meteor = evaluate.load("meteor")

meteor_score = meteor.compute(predictions=predictions, references=references)

print("\nMETEOR Score:", meteor_score["meteor"])